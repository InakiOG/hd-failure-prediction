{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required packages\n",
    "#!pip3 freeze | xargs pip3 uninstall -y\n",
    "\n",
    "# %pip install torch==2.3.0+cpu torchvision==0.18.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10 ** 6\n",
    "dtype_dict = {'date': 'str', 'serial_number': 'str', 'model': 'str', 'capacity_bytes': 'int32', 'failure': 'bool', 'datacenter': 'str', 'cluster_id': 'int8', 'vault_id': 'int16', 'pod_id': 'int16', 'pod_slot_num': 'float32', 'is_legacy_format': 'bool', 'smart_1_normalized': 'float64', 'smart_1_raw': 'float64', 'smart_2_normalized': 'float64', 'smart_2_raw': 'float64', 'smart_3_normalized': 'float64', 'smart_3_raw': 'float64', 'smart_4_normalized': 'float64', 'smart_4_raw': 'float64', 'smart_5_normalized': 'float64', 'smart_5_raw': 'float64', 'smart_7_normalized': 'float64', 'smart_7_raw': 'float64', 'smart_8_normalized': 'float64', 'smart_8_raw': 'float64', 'smart_9_normalized': 'float64', 'smart_9_raw': 'float64', 'smart_10_normalized': 'float64', 'smart_10_raw': 'float64', 'smart_11_normalized': 'float64', 'smart_11_raw': 'float64', 'smart_12_normalized': 'float64', 'smart_12_raw': 'float64', 'smart_13_normalized': 'float64', 'smart_13_raw': 'float64', 'smart_15_normalized': 'float64', 'smart_15_raw': 'float64', 'smart_16_normalized': 'float64', 'smart_16_raw': 'float64', 'smart_17_normalized': 'float64', 'smart_17_raw': 'float64', 'smart_18_normalized': 'float64', 'smart_18_raw': 'float64', 'smart_22_normalized': 'float64', 'smart_22_raw': 'float64', 'smart_23_normalized': 'float64', 'smart_23_raw': 'float64', 'smart_24_normalized': 'float64', 'smart_24_raw': 'float64', 'smart_27_normalized': 'float64', 'smart_27_raw': 'float64', 'smart_71_normalized': 'float64', 'smart_71_raw': 'float64', 'smart_82_normalized': 'float64', 'smart_82_raw': 'float64', 'smart_90_normalized': 'float64', 'smart_90_raw': 'float64', 'smart_160_normalized': 'float64', 'smart_160_raw': 'float64', 'smart_161_normalized': 'float64', 'smart_161_raw': 'float64', 'smart_163_normalized': 'float64', 'smart_163_raw': 'float64', 'smart_164_normalized': 'float64', 'smart_164_raw': 'float64', 'smart_165_normalized': 'float64', 'smart_165_raw': 'float64', 'smart_166_normalized': 'float64', 'smart_166_raw': 'float64', 'smart_167_normalized': 'float64', 'smart_167_raw': 'float64', 'smart_168_normalized': 'float64', 'smart_168_raw': 'float64', 'smart_169_normalized': 'float64', 'smart_169_raw': 'float64', 'smart_170_normalized': 'float64', 'smart_170_raw': 'float64', 'smart_171_normalized': 'float64', 'smart_171_raw': 'float64', 'smart_172_normalized': 'float64', 'smart_172_raw': 'float64', 'smart_173_normalized': 'float64', 'smart_173_raw': 'float64', 'smart_174_normalized': 'float64', 'smart_174_raw': 'float64', 'smart_175_normalized': 'float64', 'smart_175_raw': 'float64', 'smart_176_normalized': 'float64', 'smart_176_raw': 'float64', 'smart_177_normalized': 'float64', 'smart_177_raw': 'float64', 'smart_178_normalized': 'float64', 'smart_178_raw': 'float64', 'smart_179_normalized': 'float64', 'smart_179_raw': 'float64', 'smart_180_normalized': 'float64', 'smart_180_raw': 'float64', 'smart_181_normalized': 'float64', 'smart_181_raw': 'float64', 'smart_182_normalized': 'float64', 'smart_182_raw': 'float64', 'smart_183_normalized': 'float64', 'smart_183_raw': 'float64', 'smart_184_normalized': 'float64', 'smart_184_raw': 'float64', 'smart_187_normalized': 'float64', 'smart_187_raw': 'float64', 'smart_188_normalized': 'float64', 'smart_188_raw': 'float64', 'smart_189_normalized': 'float64', 'smart_189_raw': 'float64', 'smart_190_normalized': 'float64', 'smart_190_raw': 'float64', 'smart_191_normalized': 'float64', 'smart_191_raw': 'float64', 'smart_192_normalized': 'float64', 'smart_192_raw': 'float64', 'smart_193_normalized': 'float64', 'smart_193_raw': 'float64', 'smart_194_normalized': 'float64', 'smart_194_raw': 'float64', 'smart_195_normalized': 'float64', 'smart_195_raw': 'float64', 'smart_196_normalized': 'float64', 'smart_196_raw': 'float64', 'smart_197_normalized': 'float64', 'smart_197_raw': 'float64', 'smart_198_normalized': 'float64', 'smart_198_raw': 'float64', 'smart_199_normalized': 'float64', 'smart_199_raw': 'float64', 'smart_200_normalized': 'float64', 'smart_200_raw': 'float64', 'smart_201_normalized': 'float64', 'smart_201_raw': 'float64', 'smart_202_normalized': 'float64', 'smart_202_raw': 'float64', 'smart_206_normalized': 'float64', 'smart_206_raw': 'float64', 'smart_210_normalized': 'float64', 'smart_210_raw': 'float64', 'smart_218_normalized': 'float64', 'smart_218_raw': 'float64', 'smart_220_normalized': 'float64', 'smart_220_raw': 'float64', 'smart_222_normalized': 'float64', 'smart_222_raw': 'float64', 'smart_223_normalized': 'float64', 'smart_223_raw': 'float64', 'smart_224_normalized': 'float64', 'smart_224_raw': 'float64', 'smart_225_normalized': 'float64', 'smart_225_raw': 'float64', 'smart_226_normalized': 'float64', 'smart_226_raw': 'float64', 'smart_230_normalized': 'float64', 'smart_230_raw': 'float64', 'smart_231_normalized': 'float64', 'smart_231_raw': 'float64', 'smart_232_normalized': 'float64', 'smart_232_raw': 'float64', 'smart_233_normalized': 'float64', 'smart_233_raw': 'float64', 'smart_234_normalized': 'float64', 'smart_234_raw': 'float64', 'smart_235_normalized': 'float64', 'smart_235_raw': 'float64', 'smart_240_normalized': 'float64', 'smart_240_raw': 'float64', 'smart_241_normalized': 'float64', 'smart_241_raw': 'float64', 'smart_242_normalized': 'float64', 'smart_242_raw': 'float64', 'smart_244_normalized': 'float64', 'smart_244_raw': 'float64', 'smart_245_normalized': 'float64', 'smart_245_raw': 'float64', 'smart_246_normalized': 'float64', 'smart_246_raw': 'float64', 'smart_247_normalized': 'float64', 'smart_247_raw': 'float64', 'smart_248_normalized': 'float64', 'smart_248_raw': 'float64', 'smart_250_normalized': 'float64', 'smart_250_raw': 'float64', 'smart_251_normalized': 'float64', 'smart_251_raw': 'float64', 'smart_252_normalized': 'float64', 'smart_252_raw': 'float64', 'smart_254_normalized': 'float64', 'smart_254_raw': 'float64', 'smart_255_normalized': 'float64', 'smart_255_raw': 'float64'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_drives(df):\n",
    "    df.head()\n",
    "    columns_to_delete = ['model','capacity_bytes','datacenter','cluster_id','vault_id','pod_id','pod_slot_num','is_legacy_format']\n",
    "    smart_allowed = ['failure', 'serial_number']\n",
    "    for column in df.columns:\n",
    "        if column not in smart_allowed:\n",
    "            columns_to_delete.append(column)\n",
    "    df = df.drop(columns=columns_to_delete)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_failed_drives(file_path):\n",
    "    aggregated_result = pd.DataFrame() \n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize, dtype=dtype_dict):\n",
    "        chunk = clean_data_drives(chunk)\n",
    "        aggregated_result = pd.concat([aggregated_result, chunk[chunk['failure'] == 1]])\n",
    "    return aggregated_result\n",
    "\n",
    "def get_success_drives(file_path):\n",
    "    aggregated_result = pd.DataFrame() \n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize, dtype=dtype_dict):\n",
    "        chunk = clean_data_drives(chunk)\n",
    "        aggregated_result = pd.concat([aggregated_result, chunk[chunk['failure'] == 0]])\n",
    "    return aggregated_result\n",
    "\n",
    "def get_data_drives(folder_path, failed):\n",
    "    df = pd.DataFrame()\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            if failed:\n",
    "                df = pd.concat([df, get_failed_drives(file_path)])\n",
    "            else:\n",
    "                df = pd.concat([df, get_success_drives(file_path)])\n",
    "            #print(file_path, ' done')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_chunks(file_path):\n",
    "#     aggregated_result = pd.DataFrame() \n",
    "#     for chunk in pd.read_csv(file_path, chunksize=chunksize, dtype=dtype_dict):\n",
    "#         chunk = cleandata_smart(chunk)\n",
    "#         aggregated_result = pd.concat([aggregated_result, chunk])\n",
    "\n",
    "#     return aggregated_result\n",
    "\n",
    "def cleandata_smart(df, failure_array, success_array):\n",
    "    \n",
    "    df.head()\n",
    "    columns_to_delete = ['model','capacity_bytes','datacenter','cluster_id','vault_id','pod_id','pod_slot_num','is_legacy_format']\n",
    "    smart_allowed = ['date', 'serial_number']\n",
    "    rows_allowed = [1, 3, 5, 7, 9, 187, 189, 190, 195, 197] # https://www.cropel.com/library/smart-attribute-list.aspx (Interestingly the chosen attributes from RNN paper and CT paper are the same)\n",
    "    for i in rows_allowed: smart_allowed.append(f'smart_{i}_normalized')\n",
    "    for column in df.columns:\n",
    "        if column != 'failure' and column not in smart_allowed and column != \"smart_5_raw\" and column != \"smart_197_raw\":\n",
    "            columns_to_delete.append(column)\n",
    "\n",
    "\n",
    "    df = df[df['serial_number'].isin(failure_array) | df['serial_number'].isin(success_array)]\n",
    "\n",
    "    df = df.drop(columns=columns_to_delete)\n",
    "    df = df.fillna(0)\n",
    "    float_columns = df.select_dtypes(include=['float64']).columns\n",
    "    df[float_columns] = df[float_columns].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def get_failed(file_path, failure_array, success_array):\n",
    "    aggregated_result = pd.DataFrame() \n",
    "\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunksize, dtype=dtype_dict):\n",
    "        chunk = cleandata_smart(chunk, failure_array, success_array)\n",
    "        aggregated_result = pd.concat([aggregated_result, chunk])\n",
    "        \n",
    "    return aggregated_result\n",
    "\n",
    "\n",
    "def get_data(folder_path, verbose):\n",
    "    failure_array = get_data_drives(folder_path, True)['serial_number'].values\n",
    "    success_size = len(failure_array)\n",
    "    success_array = get_data_drives(folder_path, False).head(success_size)['serial_number'].values\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            df = pd.concat([df, get_failed(file_path, failure_array, success_array)])\n",
    "            #df = pd.concat([df, process_chunks(file_path)])\n",
    "            if verbose:\n",
    "                print(file_path, ' done')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(df):\n",
    "  df.sort_values(by=['serial_number', 'date'], ascending=[True, True], inplace=True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDrives(Dataset):\n",
    "    def __init__(self, day_num = None, folder_path = None, verbose = False):\n",
    "        self.day_num = day_num\n",
    "        self.folder_path = folder_path\n",
    "        self.verbose = verbose\n",
    "\n",
    "        original_df = get_data(folder_path, verbose)\n",
    "        df = process_data(original_df)\n",
    "        df = df.groupby('serial_number').filter(lambda x: len(x) > day_num)\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        # column numbers (days) (each day is an array of the smart_data) * number of drives (serial_numbers)\n",
    "        return self.day_num * len(self.df['serial_number'].unique())\n",
    "\n",
    "    def __getitem__(self, serial_number, idx):\n",
    "        # Return the date of today and the day to be predicted\n",
    "        # TODO improve the method\n",
    "        return self.df[self.df['serial_number'] == serial_number].iloc[idx], self.df[self.df['serial_number'] == serial_number].iloc[idx + 1]\n",
    "\n",
    "    def gettestandtraindata(self):\n",
    "        # Return the test and train data``\n",
    "        train_ratio = 0.8\n",
    "\n",
    "        train_df = self.df.groupby('serial_number').apply(lambda x: x.head(int(len(x) * train_ratio))).reset_index(drop=True)\n",
    "        test_df = self.df.groupby('serial_number').apply(lambda x: x.tail(int(len(x) * (1 - train_ratio)))).reset_index(drop=True)\n",
    "        return train_df, test_df\n",
    "\n",
    "    def get_df(self):\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data_Q4_2023_test'\n",
    "test_path = 'data_2023_2'\n",
    "\n",
    "custom_drives = CustomDrives(day_num = 10, folder_path=path)\n",
    "\n",
    "training_data, test_data = custom_drives.gettestandtraindata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'groupby'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m grouped_df \u001b[38;5;241m=\u001b[39m custom_drives\u001b[38;5;241m.\u001b[39mget_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mserial_number\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, group \u001b[38;5;129;01min\u001b[39;00m grouped_df:\n\u001b[0;32m      3\u001b[0m   plt\u001b[38;5;241m.\u001b[39mplot(group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m], group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmart_1_normalized\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'groupby'"
     ]
    }
   ],
   "source": [
    "grouped_df = custom_drives.get_df.groupby('serial_number')\n",
    "for name, group in grouped_df:\n",
    "  plt.plot(group['date'], group['smart_1_normalized'])\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.ylabel('smart 1')\n",
    "\n",
    "plt.title('Data by Drive')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "\n",
    "train_df = df.groupby('serial_number').apply(lambda x: x.head(int(len(x) * train_ratio))).reset_index(drop=True)\n",
    "test_df = df.groupby('serial_number').apply(lambda x: x.tail(int(len(x) * (1 - train_ratio)))).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(165156, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = train_df.iloc[:, 3:]\n",
    "dataset_train = np.reshape(dataset_train, (-1,1))\n",
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39060, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset_test = test_df.iloc[:, 3:]\n",
    "\n",
    "dataset_test = np.reshape(dataset_test, (-1,1))\n",
    "dataset_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale the data to have better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00160359]\n",
      " [0.00160359]\n",
      " [0.00160359]\n",
      " [0.        ]\n",
      " [0.00160359]]\n",
      "[0.00163913] [0.00163913] [0.00163913] [0.] [0.00163913]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# scaling dataset\n",
    "scaled_train = scaler.fit_transform(dataset_train)\n",
    "\n",
    "print(scaled_train[:5])\n",
    "# Normalizing values between 0 and 1\n",
    "scaled_test = scaler.fit_transform(dataset_test)\n",
    "print(*scaled_test[:5]) #prints the first 5 rows of scaled_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://medium.com/@VersuS_/coding-a-recurrent-neural-network-rnn-from-scratch-using-pytorch-a6c9fc8ed4a7\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/implementing-recurrent-neural-networks-in-pytorch/\n",
    "\n",
    "Reference: https://www.geeksforgeeks.org/time-series-forecasting-using-pytorch/\n",
    "\n",
    "Reference: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
